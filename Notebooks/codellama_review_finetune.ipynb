{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning CodeLlama 7B for Code Review using Unsloth on Google Colab\n",
    "\n",
    "This notebook guides you through fine-tuning the `codellama/CodeLlama-7b-hf` model to act as a code reviewing tool. We will use the Unsloth library for efficient, memory-optimized training (specifically 4-bit QLoRA) suitable for a free Google Colab environment (like the T4 GPU).\n",
    "\n",
    "**Dataset:** We will use the `HuggingFaceH4/Code-Feedback` dataset, which contains dialogues involving code snippets and feedback, making it suitable for training a code review assistant.\n",
    "\n",
    "**Goal:** To create a model that can take a piece of code as input and provide constructive review comments or suggestions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment\n",
    "\n",
    "First, we need to install the necessary libraries. Unsloth handles the installation of optimized versions of `transformers`, `peft`, `accelerate`, and `bitsandbytes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Unsloth and other required libraries\n",
    "!pip install \"unsloth[colab-new]>=2024.5\" -q\n",
    "!pip install \"transformers>=4.38.0\" -q\n",
    "!pip install \"datasets[vision]>=2.16.0\" -q\n",
    "!pip install \"accelerate>=0.28.0\" -q\n",
    "!pip install \"trl>=0.8.6\" -q\n",
    "!pip install \"peft>=0.10.0\" -q\n",
    "!pip install \"bitsandbytes>=0.43.0\" -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Check\n",
    "\n",
    "Let's verify that we have a suitable GPU available. Unsloth is optimized for NVIDIA GPUs, and free Colab typically provides a T4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU status\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries\n",
    "\n",
    "Now, import the necessary components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import TrainingArguments, AutoTokenizer\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig\n",
    "from unsloth import FastLanguageModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model and Tokenizer\n",
    "\n",
    "We'll load the `codellama/CodeLlama-7b-hf` model using Unsloth's `FastLanguageModel`. This automatically applies optimizations like 4-bit quantization (QLoRA) to make it fit within Colab's memory limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 2048 # Choose based on GPU memory and typical code review context length\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere\n",
    "load_in_4bit = True # Use 4-bit quantization to reduce memory usage\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"codellama/CodeLlama-7b-hf\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # Optional: use if accessing gated models like Llama 3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure PEFT (LoRA)\n",
    "\n",
    "We configure Parameter-Efficient Fine-Tuning (PEFT) using LoRA (Low-Rank Adaptation). Unsloth helps automatically find the optimal modules to target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Suggested rank\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"], # Modules to apply LoRA to\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing = True, # Significantly saves memory\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Test Model Before Fine-tuning\n",
    "\n",
    "Let's test the model's performance before fine-tuning to see how it responds to code review requests. This will serve as a baseline for comparison after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model before fine-tuning\n",
    "print(\"Testing model BEFORE fine-tuning...\")\n",
    "\n",
    "# Define the same prompt template we'll use for training\n",
    "test_prompt_template = \"\"\"Below is a conversation between a user asking for code review and an AI assistant providing feedback.\n",
    "### Conversation:\n",
    "{}\n",
    "### Feedback:\n",
    "\"\"\"\n",
    "\n",
    "# Example code snippet for review (same as we'll use later for comparison)\n",
    "test_code = \"\"\"\n",
    "**User:** Can you review this Python function?\n",
    "\n",
    "```python\n",
    "def add_numbers(a, b):\n",
    "  # This function adds two numbers\n",
    "  result = a+b\n",
    "  return result\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "# Format the input\n",
    "test_input = test_prompt_template.format(test_code.strip())\n",
    "test_inputs = tokenizer([test_input], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Generate response before fine-tuning\n",
    "print(\"Generating response before fine-tuning...\")\n",
    "with torch.no_grad():\n",
    "    test_outputs = model.generate(\n",
    "        **test_inputs, \n",
    "        max_new_tokens=256, \n",
    "        use_cache=True,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "# Decode the output\n",
    "test_decoded = tokenizer.batch_decode(test_outputs)[0]\n",
    "\n",
    "# Extract only the generated feedback part\n",
    "feedback_start = test_decoded.find(\"### Feedback:\") + len(\"### Feedback:\")\n",
    "generated_before = test_decoded[feedback_start:].strip()\n",
    "\n",
    "# Clean up the output\n",
    "if generated_before.endswith(tokenizer.eos_token):\n",
    "    generated_before = generated_before[:-len(tokenizer.eos_token)].strip()\n",
    "\n",
    "print(\"--- Input Code ---\")\n",
    "print(test_code)\n",
    "print(\"--- Generated Feedback (BEFORE fine-tuning) ---\")\n",
    "print(generated_before)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Now proceeding with fine-tuning...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Prepare Dataset\n",
    "\n",
    "We load the `HuggingFaceH4/Code-Feedback` dataset and format it into a structure suitable for supervised fine-tuning (SFT). We'll create a simple prompt template that presents the code and asks for a review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a prompt template for code review\n",
    "# The dataset has 'instruction', 'output', and 'messages' fields.\n",
    "# We'll use the 'messages' field which contains a list of turns.\n",
    "# We format it into a single string resembling a conversation.\n",
    "\n",
    "prompt_template = \"\"\"Below is a conversation between a user asking for code review and an AI assistant providing feedback.\n",
    "### Conversation:\n",
    "{}\n",
    "### Feedback:\n",
    "{}\n",
    "\"\"\"\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    formatted_texts = []\n",
    "    for i in range(len(examples['messages'])):\n",
    "        conversation = \"\"\n",
    "        messages = examples['messages'][i]\n",
    "        # Find the last assistant message as the target output\n",
    "        if messages[-1]['role'] == 'assistant':\n",
    "            output = messages[-1]['content']\n",
    "            # Concatenate previous messages for context\n",
    "            for msg in messages[:-1]:\n",
    "                conversation += f\"**{msg['role'].capitalize()}:** {msg['content']}\\n\"\n",
    "            conversation = conversation.strip()\n",
    "            # Apply the template\n",
    "            text = prompt_template.format(conversation, output) + EOS_TOKEN\n",
    "            formatted_texts.append(text)\n",
    "        # Handle cases where the last message isn't from the assistant (optional, could skip)\n",
    "        # else: \n",
    "        #    pass \n",
    "            \n",
    "    return { \"text\" : formatted_texts }\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"HuggingFaceH4/Code-Feedback\", split = \"train_sft\") # Using the SFT split\n",
    "\n",
    "# Apply the formatting function\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
    "\n",
    "# Optional: Shuffle and select a subset for faster training if needed\n",
    "# dataset = dataset.shuffle(seed=42).select(range(1000)) \n",
    "\n",
    "print(f\"Dataset prepared. Example entry:{dataset[0]['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configure Training\n",
    "\n",
    "Set up the training arguments using `transformers.TrainingArguments` and initialize the `SFTTrainer` from the `trl` library. We configure parameters suitable for a free Colab instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\", # Column containing the formatted text\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2, # Number of processes for dataset preparation\n",
    "    packing = False, # Can make training faster, but requires more memory\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2, # Adjust based on GPU memory\n",
    "        gradient_accumulation_steps = 4, # Effective batch size = batch_size * accumulation_steps\n",
    "        warmup_steps = 5,\n",
    "        # max_steps = 60, # Set a fixed number of steps for faster training (adjust as needed)\n",
    "        num_train_epochs = 1, # Or train for a number of epochs\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not torch.cuda.is_bf16_supported(), # Use fp16 if bf16 not supported (T4)\n",
    "        bf16 = torch.cuda.is_bf16_supported(), # Use bf16 if supported (Ampere)\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\", # Use 8-bit AdamW optimizer to save memory\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\", # Directory to save checkpoints and logs\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train the Model\n",
    "\n",
    "Start the fine-tuning process. This may take some time depending on the dataset size and `max_steps`/`num_train_epochs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training...\")\n",
    "trainer_stats = trainer.train()\n",
    "print(\"Training finished.\")\n",
    "# You can view training metrics in trainer_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save the Model Adapters\n",
    "\n",
    "After training, save the learned LoRA adapters. These adapters are much smaller than the full model and contain the fine-tuned knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the LoRA adapters\n",
    "output_dir = \"codellama_code_review_lora\"\n",
    "model.save_pretrained(output_dir) # Saves LoRA adapters\n",
    "tokenizer.save_pretrained(output_dir) # Saves tokenizer\n",
    "print(f\"LoRA adapters saved to {output_dir}\")\n",
    "\n",
    "# Optional: Push to Hugging Face Hub\n",
    "# from huggingface_hub import login\n",
    "# login() # Log in to your Hugging Face account\n",
    "# model.push_to_hub(\"your_username/codellama_code_review_lora\", token = True)\n",
    "# tokenizer.push_to_hub(\"your_username/codellama_code_review_lora\", token = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Inference Example\n",
    "\n",
    "Let's see how to use the fine-tuned model for inference. We load the base model again and apply the saved LoRA adapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# Load the base model and tokenizer again (if needed, or use the 'model' object if still in memory)\n",
    "# Make sure to provide the path to your saved adapters\n",
    "\n",
    "# Check if 'model' and 'tokenizer' are still loaded, otherwise reload\n",
    "if 'model' not in locals():\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"codellama_code_review_lora\", # Load from your saved directory\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "    print(\"Model loaded from saved adapters.\")\n",
    "else:\n",
    "    # If model is still in memory, ensure it's in evaluation mode\n",
    "    model.eval()\n",
    "    print(\"Using model already in memory.\")\n",
    "\n",
    "# Define the inference prompt template (should match the training format)\n",
    "inference_prompt_template = \"\"\"Below is a conversation between a user asking for code review and an AI assistant providing feedback.\n",
    "### Conversation:\n",
    "{}\n",
    "### Feedback:\n",
    "\"\"\"\n",
    "\n",
    "# Example code snippet for review\n",
    "code_to_review = \"\"\"\n",
    "**User:** Can you review this Python function?\n",
    "\n",
    "```python\n",
    "def add_numbers(a, b):\n",
    "  # This function adds two numbers\n",
    "  result = a+b\n",
    "  return result\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "# Format the input\n",
    "input_text = inference_prompt_template.format(code_to_review.strip())\n",
    "inputs = tokenizer([input_text], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "# Generate the feedback\n",
    "print(\"Generating feedback...\")\n",
    "outputs = model.generate(**inputs, max_new_tokens = 256, use_cache = True)\n",
    "decoded_output = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "# Extract only the generated feedback part\n",
    "feedback_start = decoded_output.find(\"### Feedback:\") + len(\"### Feedback:\")\n",
    "generated_feedback = decoded_output[feedback_start:].strip()\n",
    "# Remove potential EOS token if present at the end\n",
    "if generated_feedback.endswith(tokenizer.eos_token):\n",
    "    generated_feedback = generated_feedback[:-len(tokenizer.eos_token)].strip()\n",
    "\n",
    "print(\"--- Input Code ---\")\n",
    "print(code_to_review)\n",
    "print(\"--- Generated Feedback ---\")\n",
    "print(generated_feedback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "You have successfully fine-tuned CodeLlama 7B for code review tasks using Unsloth on Google Colab!\n",
    "\n",
    "**Next Steps:**\n",
    "*   **Evaluate:** Perform more rigorous evaluation on a separate test set to measure the quality of the reviews.\n",
    "*   **Experiment:** Try different hyperparameters (learning rate, LoRA rank, epochs), datasets, or prompt formats.\n",
    "*   **Deploy:** Merge the adapters with the base model for easier deployment or use the adapters directly with PEFT.\n",
    "*   **Push to Hub:** Share your fine-tuned adapters on the Hugging Face Hub."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
